"""
–†–æ–∑—à–∏—Ä–µ–Ω–∞ –≤–µ—Ä—Å—ñ—è MapReduce –∞–Ω–∞–ª—ñ–∑—É –∑ –¥–æ–¥–∞—Ç–∫–æ–≤–∏–º–∏ –º–æ–∂–ª–∏–≤–æ—Å—Ç—è–º–∏
"""

import urllib.request
import urllib.parse
import re
import matplotlib.pyplot as plt
import concurrent.futures
from collections import defaultdict, Counter
import time
import argparse
import json
from typing import List, Dict, Tuple, Optional


class MapReduceWordAnalyzer:
    """
    –ö–ª–∞—Å –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤ –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º MapReduce
    """
    
    def __init__(self, num_workers: int = 4, min_word_length: int = 3):
        """
        –Ü–Ω—ñ—Ü—ñ–∞–ª—ñ–∑–∞—Ü—ñ—è –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä–∞
        
        Args:
            num_workers: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏
            min_word_length: –ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Å–ª–æ–≤–∞ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        """
        self.num_workers = num_workers
        self.min_word_length = min_word_length
        self.stop_words = {
            'english': {'the', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with', 'by', 'is', 'are', 'was', 'were', 'be', 'been', 'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should', 'may', 'might', 'can', 'must', 'shall', 'this', 'that', 'these', 'those', 'i', 'you', 'he', 'she', 'it', 'we', 'they', 'me', 'him', 'her', 'us', 'them', 'my', 'your', 'his', 'her', 'its', 'our', 'their', 'a', 'an'},
            'ukrainian': {'—ñ', '–≤', '–Ω–∞', '–∑', '–∑–∞', '–¥–æ', '–≤—ñ–¥', '–ø—Ä–æ', '–¥–ª—è', '—è–∫', '—â–æ', '—Ü–µ', '—Ç–æ–π', '—Ç–∞', '–∞–±–æ', '–∞–ª–µ', '–Ω–µ', '—î', '–±—É–≤', '–±—É–ª–∞', '–±—É–ª–æ', '–±—É–ª–∏', '–±—É–¥–µ', '–±—É–¥—É—Ç—å', '–º–∞—Ç–∏', '–º–∞—î', '–º–∞—é—Ç—å', '–º–∞–≤', '–º–∞–ª–∞', '–º–∞–ª–æ', '–º–∞–ª–∏', '–≤—ñ–Ω', '–≤–æ–Ω–∞', '–≤–æ–Ω–æ', '–≤–æ–Ω–∏', '—è', '—Ç–∏', '–º–∏', '–≤–∏', '–º–µ–Ω–µ', '—Ç–µ–±–µ', '–π–æ–≥–æ', '—ó—ó', '–Ω–∞—Å', '–≤–∞—Å', '—ó—Ö', '–º—ñ–π', '—Ç–≤—ñ–π', '–Ω–∞—à', '–≤–∞—à', '—ó—Ö–Ω—ñ–π'}
        }
    
    def fetch_text_from_url(self, url: str) -> str:
        """
        –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç –∑ URL –∑ –æ–±—Ä–æ–±–∫–æ—é –ø–æ–º–∏–ª–æ–∫
        """
        try:
            # –î–æ–¥–∞—î–º–æ User-Agent –¥–ª—è —É–Ω–∏–∫–Ω–µ–Ω–Ω—è –±–ª–æ–∫—É–≤–∞–Ω–Ω—è
            req = urllib.request.Request(url, headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            })
            
            with urllib.request.urlopen(req, timeout=30) as response:
                content = response.read()
                
                # –°–ø—Ä–æ–±—É—î–º–æ —Ä—ñ–∑–Ω—ñ –∫–æ–¥—É–≤–∞–Ω–Ω—è
                for encoding in ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']:
                    try:
                        return content.decode(encoding)
                    except UnicodeDecodeError:
                        continue
                
                # –Ø–∫—â–æ –∂–æ–¥–Ω–µ –∫–æ–¥—É–≤–∞–Ω–Ω—è –Ω–µ —Å–ø—Ä–∞—Ü—é–≤–∞–ª–æ, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∑ —ñ–≥–Ω–æ—Ä—É–≤–∞–Ω–Ω—è–º –ø–æ–º–∏–ª–æ–∫
                return content.decode('utf-8', errors='ignore')
                
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ URL {url}: {e}")
            return ""
    
    def clean_and_tokenize(self, text: str, remove_stop_words: bool = True) -> List[str]:
        """
        –û—á–∏—â–∞—î —Ç–µ–∫—Å—Ç —Ç–∞ —Ä–æ–∑–±–∏–≤–∞—î –Ω–∞ —Ç–æ–∫–µ–Ω–∏
        """
        # –ü—Ä–∏–≤–æ–¥–∏–º–æ –¥–æ –Ω–∏–∂–Ω—å–æ–≥–æ —Ä–µ–≥—ñ—Å—Ç—Ä—É —Ç–∞ –≤–∏—Ç—è–≥—É—î–º–æ —Å–ª–æ–≤–∞
        words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å—ñ–Ü—ó–á—î–Ñ]+\b', text.lower())
        
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –∑–∞ –¥–æ–≤–∂–∏–Ω–æ—é
        words = [word for word in words if len(word) >= self.min_word_length]
        
        # –í–∏–¥–∞–ª—è—î–º–æ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
        if remove_stop_words:
            all_stop_words = set()
            for stop_words in self.stop_words.values():
                all_stop_words.update(stop_words)
            words = [word for word in words if word not in all_stop_words]
        
        return words
    
    def map_function(self, text_chunk: str) -> List[Tuple[str, int]]:
        """Map —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –æ–±—Ä–æ–±–∫–∏ —á–∞—Å—Ç–∏–Ω–∏ —Ç–µ–∫—Å—Ç—É"""
        words = self.clean_and_tokenize(text_chunk)
        return [(word, 1) for word in words]
    
    def reduce_function(self, mapped_results: List[List[Tuple[str, int]]]) -> Dict[str, int]:
        """Reduce —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è –∞–≥—Ä–µ–≥–∞—Ü—ñ—ó —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ñ–≤"""
        word_count = defaultdict(int)
        
        for word_list in mapped_results:
            for word, count in word_list:
                word_count[word] += count
        
        return dict(word_count)
    
    def split_text_into_chunks(self, text: str) -> List[str]:
        """–†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏"""
        if not text:
            return []
        
        chunk_size = max(len(text) // self.num_workers, 1000)  # –ú—ñ–Ω—ñ–º—É–º 1000 —Å–∏–º–≤–æ–ª—ñ–≤
        chunks = []
        
        for i in range(self.num_workers):
            start = i * chunk_size
            if i == self.num_workers - 1:
                end = len(text)
            else:
                end = (i + 1) * chunk_size
                # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –Ω–∞–π–±–ª–∏–∂—á–∏–π –ø—Ä–æ–±—ñ–ª
                while end < len(text) and text[end] != ' ':
                    end += 1
            
            if start < len(text):
                chunks.append(text[start:end])
        
        return chunks
    
    def analyze_text(self, text: str) -> Dict[str, int]:
        """
        –í–∏–∫–æ–Ω—É—î –ø–æ–≤–Ω–∏–π –∞–Ω–∞–ª—ñ–∑ —Ç–µ–∫—Å—Ç—É –∑ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è–º MapReduce
        """
        if not text:
            return {}
        
        print(f"–ê–Ω–∞–ª—ñ–∑—É—î–º–æ —Ç–µ–∫—Å—Ç –¥–æ–≤–∂–∏–Ω–æ—é {len(text)} —Å–∏–º–≤–æ–ª—ñ–≤...")
        start_time = time.time()
        
        # –†–æ–∑–±–∏–≤–∞—î–º–æ —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏
        text_chunks = self.split_text_into_chunks(text)
        print(f"–†–æ–∑–±–∏—Ç–æ –Ω–∞ {len(text_chunks)} —á–∞—Å—Ç–∏–Ω")
        
        # Map —Ñ–∞–∑–∞ –∑ –±–∞–≥–∞—Ç–æ–ø–æ—Ç–æ–∫–æ–≤—ñ—Å—Ç—é
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.num_workers) as executor:
            mapped_results = list(executor.map(self.map_function, text_chunks))
        
        # Reduce —Ñ–∞–∑–∞
        word_count = self.reduce_function(mapped_results)
        
        end_time = time.time()
        print(f"–ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {end_time - start_time:.2f} —Å–µ–∫—É–Ω–¥")
        print(f"–ó–Ω–∞–π–¥–µ–Ω–æ {len(word_count)} —É–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤")
        
        return word_count
    
    def save_results(self, word_count: Dict[str, int], filename: str = "word_analysis.json"):
        """–ó–±–µ—Ä—ñ–≥–∞—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —É —Ñ–∞–π–ª"""
        try:
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(word_count, f, ensure_ascii=False, indent=2)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —É —Ñ–∞–π–ª {filename}")
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è: {e}")
    
    def load_results(self, filename: str = "word_analysis.json") -> Optional[Dict[str, int]]:
        """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ –∑ —Ñ–∞–π–ª—É"""
        try:
            with open(filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è: {e}")
            return None


def visualize_top_words(word_count: Dict[str, int], top_n: int = 10, save_plot: bool = False):
    """
    –†–æ–∑—à–∏—Ä–µ–Ω–∞ –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—è –∑ –º–æ–∂–ª–∏–≤—ñ—Å—Ç—é –∑–±–µ—Ä–µ–∂–µ–Ω–Ω—è
    """
    if not word_count:
        print("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó")
        return
    
    # –û—Ç—Ä–∏–º—É—î–º–æ —Ç–æ–ø —Å–ª–æ–≤–∞
    top_words = Counter(word_count).most_common(top_n)
    
    if not top_words:
        print("–ù–µ –∑–Ω–∞–π–¥–µ–Ω–æ —Å–ª—ñ–≤ –¥–ª—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó")
        return
    
    words, counts = zip(*top_words)
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –≥—Ä–∞—Ñ—ñ–∫ –∑ –ø–æ–∫—Ä–∞—â–µ–Ω–∏–º –¥–∏–∑–∞–π–Ω–æ–º
    plt.figure(figsize=(14, 8))
    colors = plt.cm.viridis(range(len(words)))
    bars = plt.bar(range(len(words)), counts, color=colors, edgecolor='black', alpha=0.8)
    
    # –ù–∞–ª–∞—à—Ç–æ–≤—É—î–º–æ –≥—Ä–∞—Ñ—ñ–∫
    plt.title(f'–¢–æ–ø {top_n} –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –≤–∂–∏–≤–∞–Ω–∏—Ö —Å–ª—ñ–≤\n(–ó–∞–≥–∞–ª–æ–º –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑–æ–≤–∞–Ω–æ {sum(word_count.values())} —Å–ª—ñ–≤)', 
              fontsize=16, fontweight='bold', pad=20)
    plt.xlabel('–°–ª–æ–≤–∞', fontsize=12, fontweight='bold')
    plt.ylabel('–ß–∞—Å—Ç–æ—Ç–∞ –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è', fontsize=12, fontweight='bold')
    plt.xticks(range(len(words)), words, rotation=45, ha='right', fontsize=10)
    
    # –î–æ–¥–∞—î–º–æ –∑–Ω–∞—á–µ–Ω–Ω—è –Ω–∞ —Å—Ç–æ–≤–ø—Ü—ñ
    for bar, count in zip(bars, counts):
        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(counts)*0.01,
                str(count), ha='center', va='bottom', fontweight='bold', fontsize=9)
    
    # –ü–æ–∫—Ä–∞—â—É—î–º–æ –≤–∏–≥–ª—è–¥
    plt.tight_layout()
    plt.grid(axis='y', alpha=0.3, linestyle='--')
    
    # –î–æ–¥–∞—î–º–æ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    total_words = sum(word_count.values())
    unique_words = len(word_count)
    coverage = sum(counts) / total_words * 100
    
    stats_text = f'–£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {unique_words}\n–í—Å—å–æ–≥–æ —Å–ª—ñ–≤: {total_words}\n–ü–æ–∫—Ä–∏—Ç—Ç—è —Ç–æ–ø-{top_n}: {coverage:.1f}%'
    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, 
             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='lightgray', alpha=0.8))
    
    if save_plot:
        plt.savefig('word_frequency_analysis.png', dpi=300, bbox_inches='tight')
        print("–ì—Ä–∞—Ñ—ñ–∫ –∑–±–µ—Ä–µ–∂–µ–Ω–æ —è–∫ 'word_frequency_analysis.png'")
    
    plt.show()
    
    # –í–∏–≤–æ–¥–∏–º–æ –¥–µ—Ç–∞–ª—å–Ω—ñ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print(f"\n–î–µ—Ç–∞–ª—å–Ω–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print("=" * 50)
    print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤: {total_words}")
    print(f"–£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {unique_words}")
    print(f"–°–µ—Ä–µ–¥–Ω—è —á–∞—Å—Ç–æ—Ç–∞ –Ω–∞ —Å–ª–æ–≤–æ: {total_words / unique_words:.2f}")
    print(f"\n–¢–æ–ø {top_n} –Ω–∞–π—á–∞—Å—Ç—ñ—à–µ –≤–∂–∏–≤–∞–Ω–∏—Ö —Å–ª—ñ–≤:")
    print("-" * 40)
    for i, (word, count) in enumerate(top_words, 1):
        percentage = count / total_words * 100
        print(f"{i:2d}. {word:<20} - {count:>6} —Ä–∞–∑—ñ–≤ ({percentage:5.2f}%)")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –∑ –ø—ñ–¥—Ç—Ä–∏–º–∫–æ—é –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ –∫–æ–º–∞–Ω–¥–Ω–æ–≥–æ —Ä—è–¥–∫–∞"""
    parser = argparse.ArgumentParser(description='MapReduce Word Frequency Analysis')
    parser.add_argument('--url', type=str, 
                       default='https://www.gutenberg.org/files/74/74-0.txt',
                       help='URL –¥–ª—è –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É')
    parser.add_argument('--workers', type=int, default=4,
                       help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø–æ—Ç–æ–∫—ñ–≤ –¥–ª—è –æ–±—Ä–æ–±–∫–∏')
    parser.add_argument('--top', type=int, default=10,
                       help='–ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–ø —Å–ª—ñ–≤ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è')
    parser.add_argument('--min-length', type=int, default=3,
                       help='–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Å–ª–æ–≤–∞')
    parser.add_argument('--save', action='store_true',
                       help='–ó–±–µ—Ä–µ–≥—Ç–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —Ç–∞ –≥—Ä–∞—Ñ—ñ–∫')
    
    args = parser.parse_args()
    
    # –°—Ç–≤–æ—Ä—é—î–º–æ –∞–Ω–∞–ª—ñ–∑–∞—Ç–æ—Ä
    analyzer = MapReduceWordAnalyzer(
        num_workers=args.workers,
        min_word_length=args.min_length
    )
    
    print("üöÄ –ó–∞–ø—É—Å–∫ MapReduce –∞–Ω–∞–ª—ñ–∑—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤")
    print("=" * 50)
    print(f"URL: {args.url}")
    print(f"–ü–æ—Ç–æ–∫—ñ–≤: {args.workers}")
    print(f"–ú—ñ–Ω—ñ–º–∞–ª—å–Ω–∞ –¥–æ–≤–∂–∏–Ω–∞ —Å–ª–æ–≤–∞: {args.min_length}")
    print(f"–¢–æ–ø —Å–ª—ñ–≤ –¥–ª—è –≤—ñ–¥–æ–±—Ä–∞–∂–µ–Ω–Ω—è: {args.top}")
    print("=" * 50)
    
    # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–µ–∫—Å—Ç
    print("\nüì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É...")
    text = analyzer.fetch_text_from_url(args.url)
    
    if not text:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π...")
        text = """
        MapReduce is a programming model and implementation for processing and generating large datasets.
        The MapReduce framework provides a simple interface for implementing distributed computations.
        MapReduce allows programmers to express computations as map and reduce operations.
        The framework automatically handles the parallelization, fault tolerance, and load balancing.
        This makes MapReduce very popular for big data processing in distributed systems.
        Many companies use MapReduce for analyzing large amounts of data efficiently.
        The map function processes input data and produces intermediate key-value pairs.
        The reduce function aggregates the intermediate values associated with the same key.
        Together, map and reduce operations enable powerful data analysis capabilities.
        """
    
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(text):,} —Å–∏–º–≤–æ–ª—ñ–≤")
    
    # –í–∏–∫–æ–Ω—É—î–º–æ –∞–Ω–∞–ª—ñ–∑
    print("\n‚öôÔ∏è –í–∏–∫–æ–Ω–∞–Ω–Ω—è MapReduce –∞–Ω–∞–ª—ñ–∑—É...")
    word_count = analyzer.analyze_text(text)
    
    if not word_count:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –ø—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç")
        return
    
    # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏ —è–∫—â–æ –ø–æ—Ç—Ä—ñ–±–Ω–æ
    if args.save:
        analyzer.save_results(word_count)
    
    # –í—ñ–∑—É–∞–ª—ñ–∑—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    print(f"\nüìä –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –≤—ñ–∑—É–∞–ª—ñ–∑–∞—Ü—ñ—ó —Ç–æ–ø-{args.top} —Å–ª—ñ–≤...")
    visualize_top_words(word_count, top_n=args.top, save_plot=args.save)
    
    print("\n‚úÖ –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø—ñ—à–Ω–æ!")


if __name__ == "__main__":
    main()
