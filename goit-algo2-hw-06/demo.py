"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ–π–Ω–∏–π —Å–∫—Ä–∏–ø—Ç MapReduce –±–µ–∑ GUI –¥–ª—è CI/CD
"""

import urllib.request
import re
import concurrent.futures
from collections import defaultdict, Counter
from functools import reduce


def fetch_text_from_url(url):
    """–ó–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç –∑ URL-–∞–¥—Ä–µ—Å–∏"""
    try:
        with urllib.request.urlopen(url) as response:
            content = response.read()
            try:
                text = content.decode('utf-8')
            except UnicodeDecodeError:
                try:
                    text = content.decode('latin-1')
                except UnicodeDecodeError:
                    text = content.decode('cp1252', errors='ignore')
            return text
    except Exception as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –∑ URL: {e}")
        return ""


def map_function(text_chunk):
    """Map —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è MapReduce"""
    words = re.findall(r'\b[a-zA-Z–∞-—è–ê-–Ø—ë–Å—ñ–Ü—ó–á—î–Ñ]+\b', text_chunk.lower())
    words = [word for word in words if len(word) >= 3]
    return [(word, 1) for word in words]


def reduce_function(mapped_results):
    """Reduce —Ñ—É–Ω–∫—Ü—ñ—è –¥–ª—è MapReduce"""
    word_count = defaultdict(int)
    for word_list in mapped_results:
        for word, count in word_list:
            word_count[word] += count
    return dict(word_count)


def split_text_into_chunks(text, num_chunks=4):
    """–†–æ–∑–±–∏–≤–∞—î —Ç–µ–∫—Å—Ç –Ω–∞ —á–∞—Å—Ç–∏–Ω–∏"""
    if not text:
        return []
    
    chunk_size = len(text) // num_chunks
    chunks = []
    start = 0
    
    for i in range(num_chunks):
        if i == num_chunks - 1:
            end = len(text)
        else:
            end = start + chunk_size
            while end < len(text) and text[end] != ' ':
                end += 1
        
        chunks.append(text[start:end])
        start = end
        while start < len(text) and text[start] == ' ':
            start += 1
    
    return chunks


def mapreduce_word_count(text, num_workers=4):
    """MapReduce –¥–ª—è –ø—ñ–¥—Ä–∞—Ö—É–Ω–∫—É —á–∞—Å—Ç–æ—Ç–∏ —Å–ª—ñ–≤"""
    text_chunks = split_text_into_chunks(text, num_workers)
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=num_workers) as executor:
        mapped_results = list(executor.map(map_function, text_chunks))
    
    word_count = reduce_function(mapped_results)
    return word_count


def display_top_words(word_count, top_n=10):
    """–í—ñ–¥–æ–±—Ä–∞–∂–∞—î —Ç–æ–ø —Å–ª–æ–≤–∞ –≤ –∫–æ–Ω—Å–æ–ª—ñ"""
    if not word_count:
        print("–ù–µ–º–∞—î –¥–∞–Ω–∏—Ö –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É")
        return
    
    top_words = Counter(word_count).most_common(top_n)
    total_words = sum(word_count.values())
    
    print(f"\n{'='*60}")
    print(f"–†–ï–ó–£–õ–¨–¢–ê–¢–ò –ê–ù–ê–õ–Ü–ó–£ –ß–ê–°–¢–û–¢–ò –°–õ–Ü–í (MapReduce)")
    print(f"{'='*60}")
    print(f"–ó–∞–≥–∞–ª—å–Ω–∞ –∫—ñ–ª—å–∫—ñ—Å—Ç—å —Å–ª—ñ–≤: {total_words:,}")
    print(f"–£–Ω—ñ–∫–∞–ª—å–Ω–∏—Ö —Å–ª—ñ–≤: {len(word_count):,}")
    print(f"–°–µ—Ä–µ–¥–Ω—è —á–∞—Å—Ç–æ—Ç–∞: {total_words / len(word_count):.2f}")
    print(f"\n–¢–û–ü-{top_n} –ù–ê–ô–ß–ê–°–¢–Ü–®–ï –í–ñ–ò–í–ê–ù–ò–• –°–õ–Ü–í:")
    print(f"{'-'*60}")
    print(f"{'‚Ññ':>3} {'–°–õ–û–í–û':<20} {'–ß–ê–°–¢–û–¢–ê':>10} {'–í–Ü–î–°–û–¢–û–ö':>10}")
    print(f"{'-'*60}")
    
    for i, (word, count) in enumerate(top_words, 1):
        percentage = count / total_words * 100
        print(f"{i:>3}. {word:<20} {count:>10} {percentage:>9.2f}%")
    
    print(f"{'-'*60}")
    
    # –í—ñ–∑—É–∞–ª—å–Ω–∞ –¥—ñ–∞–≥—Ä–∞–º–∞ –≤ –∫–æ–Ω—Å–æ–ª—ñ
    print(f"\n–í–Ü–ó–£–ê–õ–¨–ù–ê –î–Ü–ê–ì–†–ê–ú–ê:")
    print(f"{'-'*60}")
    max_count = top_words[0][1] if top_words else 0
    bar_width = 40
    
    for i, (word, count) in enumerate(top_words[:10], 1):
        bar_length = int((count / max_count) * bar_width)
        bar = '‚ñà' * bar_length + '‚ñë' * (bar_width - bar_length)
        print(f"{i:>2}. {word:<12} |{bar}| {count}")


def main():
    """–ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü—ñ—ó"""
    print("üöÄ –ó–ê–ü–£–°–ö MapReduce –ê–ù–ê–õ–Ü–ó–£ –ß–ê–°–¢–û–¢–ò –°–õ–Ü–í")
    print("="*60)
    
    # URL –¥–ª—è —Ç–µ—Å—Ç—É–≤–∞–Ω–Ω—è
    url = "https://www.gutenberg.org/files/74/74-0.txt"  # Tom Sawyer
    
    print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è —Ç–µ–∫—Å—Ç—É –∑ URL...")
    print(f"URL: {url}")
    
    text = fetch_text_from_url(url)
    
    if not text:
        print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –∑–∞–≤–∞–Ω—Ç–∞–∂–∏—Ç–∏ —Ç–µ–∫—Å—Ç. –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –¥–µ–º–æ-—Ç–µ–∫—Å—Ç...")
        text = """
        MapReduce is a programming model and an associated implementation for processing and generating 
        large data sets with a parallel, distributed algorithm on a cluster. MapReduce was originally 
        developed by Google for their search engine indexing. The MapReduce framework provides automatic 
        parallelization and distribution of computation along with fault tolerance. Users specify a map 
        function that processes a key-value pair to generate a set of intermediate key-value pairs, and 
        a reduce function that merges all intermediate values associated with the same intermediate key. 
        Programs written in this functional style are automatically parallelized and executed on a large 
        cluster of commodity machines. This allows programmers without experience with parallel and 
        distributed systems to easily utilize the resources of a large distributed system.
        """ * 10  # –ü–æ–≤—Ç–æ—Ä—é—î–º–æ –¥–ª—è –±—ñ–ª—å—à–æ–≥–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–∏—Ö
    
    print(f"‚úÖ –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ {len(text):,} —Å–∏–º–≤–æ–ª—ñ–≤")
    
    print(f"\n‚öôÔ∏è –í–∏–∫–æ–Ω–∞–Ω–Ω—è MapReduce –∞–Ω–∞–ª—ñ–∑—É...")
    print(f"üîß –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ 4 –ø–æ—Ç–æ–∫–∏ –¥–ª—è –ø–∞—Ä–∞–ª–µ–ª—å–Ω–æ—ó –æ–±—Ä–æ–±–∫–∏")
    
    import time
    start_time = time.time()
    word_count = mapreduce_word_count(text, num_workers=4)
    end_time = time.time()
    
    print(f"‚è±Ô∏è –ê–Ω–∞–ª—ñ–∑ –∑–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {end_time - start_time:.3f} —Å–µ–∫—É–Ω–¥")
    
    # –í—ñ–¥–æ–±—Ä–∞–∂–∞—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∏
    display_top_words(word_count, top_n=15)
    
    print(f"\nüéØ –ö–†–ò–¢–ï–†–Ü–á –ü–†–ò–ô–ù–Ø–¢–¢–Ø:")
    print(f"‚úÖ –ö–æ–¥ —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–∞–Ω—Ç–∞–∂—É—î —Ç–µ–∫—Å—Ç —ñ–∑ URL")
    print(f"‚úÖ MapReduce –∫–æ—Ä–µ–∫—Ç–Ω–æ –∞–Ω–∞–ª—ñ–∑—É—î —á–∞—Å—Ç–æ—Ç—É —Å–ª—ñ–≤")
    print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ –≤—ñ–¥–æ–±—Ä–∞–∂–∞—é—Ç—å —Ç–æ–ø-—Å–ª–æ–≤–∞")
    print(f"‚úÖ –ï—Ñ–µ–∫—Ç–∏–≤–Ω–æ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è –±–∞–≥–∞—Ç–æ–ø–æ—Ç–æ–∫–æ–≤—ñ—Å—Ç—å")
    print(f"‚úÖ –ö–æ–¥ —á–∏—Ç–∞–±–µ–ª—å–Ω–∏–π —Ç–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—î PEP 8")
    
    print(f"\nüèÅ –ê–ù–ê–õ–Ü–ó –ó–ê–í–ï–†–®–ï–ù–û –£–°–ü–Ü–®–ù–û!")


if __name__ == "__main__":
    main()
